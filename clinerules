# Ballet World Project Rules and Patterns

## Project Structure

- The project follows a modular structure with separate directories for each ballet company scraper
- Common utilities are in `scrapers/common/` directory
- API server is in the `api/` directory
- Tests are in `tests/` directories within each component
- Main entry point is `run.py`

## Naming Conventions

- Python files: snake_case (e.g., `bolshoi_scraper.py`)
- Classes: PascalCase (e.g., `PerformanceScraper`)
- Functions and variables: snake_case (e.g., `scrape_performances`)
- Constants: UPPER_SNAKE_CASE (e.g., `BASE_URL`)
- MongoDB collections: snake_case (e.g., `paris_opera_ballet`)

## Code Style

- Follow PEP 8 guidelines for Python code
- Use 4 spaces for indentation in Python files
- Maximum line length is 100 characters
- Use docstrings for all modules, classes, and functions
- Include type hints where appropriate

## Data Structure

- Performance data should follow this structure:
  ```python
  {
      'title': 'Swan Lake',
      'composer': 'Pyotr Tchaikovsky',
      'date': 'December 10-15, 2025',
      'venue': 'Bolshoi Theatre',
      'description': 'A beautiful ballet...',
      'url': 'https://www.bolshoi.ru/en/performances/swan-lake',
      'company': 'Bolshoi Ballet',
      'details_scraped': True,
      'last_updated': '2025-05-17 15:00:00'
  }
  ```
- Ensure consistent field names across all scrapers
- Use ISO format for dates where possible
- Include company name in all performance records

## Error Handling

- Use try-except blocks for operations that might fail
- Log errors with appropriate level (INFO, WARNING, ERROR)
- Include context in error messages
- Implement fallbacks for missing data

## Testing

- Write unit tests for all components
- Use mocking for external dependencies
- Follow AAA pattern (Arrange, Act, Assert)
- Test both success and failure cases

## Logging

- Use the Python logging module
- Include timestamp, level, and message
- Log at appropriate levels:
  - DEBUG: Detailed debugging information
  - INFO: Confirmation that things are working as expected
  - WARNING: Something unexpected happened, but the application still works
  - ERROR: The application has failed to perform some function
  - CRITICAL: The application is unable to continue running

## API Endpoints

- Follow RESTful conventions
- Use plural nouns for resources (e.g., `/api/performances`)
- Include company ID in endpoints for company-specific resources (e.g., `/api/performances/{company_id}`)
- Support pagination with `limit` and `skip` parameters
- Return consistent JSON response structure

## Database Operations

- Use the common utilities in `scrapers/common/db.py` for database operations
- Implement upsert operations to avoid duplicates
- Include `last_updated` field in all records
- Use appropriate indexes for frequently queried fields

## Scraping Best Practices

- Implement delays between requests to avoid overloading target websites
- Use Selenium for JavaScript-heavy pages
- Handle cookie consent dialogs
- Implement robust selectors that can handle HTML structure changes
- Clean HTML from extracted text
- Provide fallback descriptions for performances without descriptions

## Command Line Interface

- Use argparse for command-line arguments
- Provide help text for all arguments
- Support common operations as subcommands
- Return appropriate exit codes (0 for success, non-zero for failure)

## Environment Variables

- Store sensitive information in environment variables
- Use python-dotenv to load environment variables from .env file
- Provide default values for non-sensitive environment variables
- Document all required environment variables

## Documentation

- Include README.md files in all directories
- Document API endpoints with examples
- Include setup instructions in the main README.md
- Document how to add new ballet companies
